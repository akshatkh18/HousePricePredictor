{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201b4c3d-1cfa-4b40-93c0-41dba1a7ab9d",
   "metadata": {},
   "source": [
    "#                                                   HOUSE PRICE PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1cf1dc-8871-4df8-b037-acd73f88b74a",
   "metadata": {},
   "source": [
    "## Frameworks Used\n",
    "\n",
    "    Pandas\n",
    "    Numpy\n",
    "    Scikitlearn\n",
    "    Matplotlib\n",
    "    pickle module\n",
    "    time module\n",
    "    sweetviz module\n",
    "    ipywidgets\n",
    "    Geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5119b6-bada-4fd8-81ea-5fca4b2b8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70875705-8dab-42ed-bb3b-ffa8a146f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing            #Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db2e8cb-c0a1-4512-980d-e5124103830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import sweetviz as sv\n",
    "    SWEETVIZ_LOADED = True\n",
    "except ImportError:\n",
    "    print(\"Sweetviz not installed. Skipping EDA report generation.\")\n",
    "    print(\"Install using: pip install sweetviz\")\n",
    "    SWEETVIZ_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ba4ffb-398c-46c3-8883-81b684941d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "    GEOPY_LOADED = True\n",
    "except ImportError:\n",
    "    print(\"Geopy not installed. Geocoding feature engineering will be skipped unless location_cache.pickle exists.\")\n",
    "    print(\"Install using: pip install geopy\")\n",
    "    GEOPY_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11bdba54-df91-4f44-af62-8c565ff18088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching California housing dataset...\n",
      "Dataset fetched.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching California housing dataset...\")\n",
    "data = fetch_california_housing()\n",
    "print(\"Dataset fetched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ea3958-f982-440b-8cf7-2e44e17ac3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)  # Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a511ee9-38b4-4e55-b726-cce1acf8b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Independent Data\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6947ce50-5251-48b2-a5da-11098e73374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent data\n",
    "df['Target'] = data.target # Target is median house value in $100,000s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36dae45-100b-4887-9ad8-362318e4b432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame head:\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  Target  \n",
      "0    -122.23   4.526  \n",
      "1    -122.22   3.585  \n",
      "2    -122.24   3.521  \n",
      "3    -122.25   3.413  \n",
      "4    -122.25   3.422  \n",
      "\n",
      "Initial DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   MedInc      20640 non-null  float64\n",
      " 1   HouseAge    20640 non-null  float64\n",
      " 2   AveRooms    20640 non-null  float64\n",
      " 3   AveBedrms   20640 non-null  float64\n",
      " 4   Population  20640 non-null  float64\n",
      " 5   AveOccup    20640 non-null  float64\n",
      " 6   Latitude    20640 non-null  float64\n",
      " 7   Longitude   20640 non-null  float64\n",
      " 8   Target      20640 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial DataFrame head:\")\n",
    "print(df.head())\n",
    "print(\"\\nInitial DataFrame info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bb85c-7d8b-4db0-b965-2d34c24565f5",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "\n",
    "\n",
    "Sweetviz is an open-source pandas-based library to perform the primary EDA task without much hassle or with just two lines of code. It also generates a summarised report with great visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c9baba-dcb4-4d89-8e78-25bb9e4bc845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Sweetviz EDA report (this may take a moment)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1bade69bea472a92fa707b58e9d01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                             |      | [  0%]   00:00 -> (? left)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report ./california_housing_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n",
      "Sweetviz report saved to ./california_housing_report.html\n"
     ]
    }
   ],
   "source": [
    "if SWEETVIZ_LOADED:\n",
    "    print(\"\\nGenerating Sweetviz EDA report (this may take a moment)...\")\n",
    "    try:\n",
    "        report = sv.analyze(df)\n",
    "        report_path = \"./california_housing_report.html\"\n",
    "        report.show_html(report_path) # Save in the form of html\n",
    "        print(f\"Sweetviz report saved to {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate Sweetviz report: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping Sweetviz EDA report generation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c991eb1-bee6-4132-b401-01b5a8d0c982",
   "metadata": {},
   "source": [
    "## Feature Engineering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b1ecea2-3b84-4942-8483-77c11bfa3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to run the time-consuming geocoding process.\n",
    "# Set to False to attempt loading pre-computed data from 'location_cache.pickle'.\n",
    "RUN_GEOCODING = False\n",
    "LOCATION_CACHE_FILE = \"location_cache.pickle\" # File to save/load geocoding results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de4385-2e88-43ac-b6a5-c4a384a5302b",
   "metadata": {},
   "source": [
    "# About Pickle Module \n",
    "\n",
    "`geopy` is a Python library that allows you to convert addresses to coordinates and converts coordinates to addresses)\n",
    "\n",
    "user_agent='house_predictor' is required because Nominatim requires a user-agent to identify the source of requests.\n",
    "\t• The user_agent can be any unique string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed83f34-b45f-415b-a00d-6490de48672c",
   "metadata": {},
   "source": [
    "The pickle module is used to serialize (save) and deserialize (load) Python objects. This is helpful when you need to:\n",
    "    \t• Store location data for future use instead of repeatedly making API calls.\n",
    "    \t• Cache results to speed up processing.\n",
    "    \t• Save and load complex Python objects (like dictionaries or lists) for later use.\n",
    "     \n",
    "If you want to save the loc.update dictionary (which stores locations) and reload it later, you can use pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1338ad7-3f92-4185-86dd-db1546fca5b6",
   "metadata": {},
   "source": [
    "## Why Use pickle Here?\n",
    "\n",
    "\tAvoid unnecessary API calls: If location data is already saved, you can load it instead of making another API request.\n",
    " \n",
    "\tPersistent storage: Data is saved even after the program exits.\n",
    " \n",
    "\tFast retrieval: Loading from a file is faster than querying an external API.\n",
    "\n",
    "\n",
    "`pickle.dump(obj, file)` → Saves (serializes) an object to a file.\n",
    "\n",
    "`pickle.load(file)` → Loads (deserializes) an object from a file.\n",
    "\n",
    "\n",
    "`wb:` Write Binary (Used for saving objects)\n",
    "`rb:`Read Binary (Used for loading objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fdf0cce-1c82-4534-8d25-0d912daca6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocoding Setup (Needed if RUN_GEOCODING is True) \n",
    "if GEOPY_LOADED:\n",
    "    geolocator = Nominatim(user_agent='house_predictor_agent_v1', timeout=10) # Added timeout\n",
    "else:\n",
    "    geolocator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff32496e-005f-41b6-ad05-cf8dd067bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geolocator.reverse(\"37.88\" + \",\" + \"-122.23\")[0]\n",
    "\n",
    "# geolocator.reverse(\"37.88\" + \",\" + \"-122.23\").raw['address']\n",
    "\n",
    "# geolocator.reverse(\"37.88\" + \",\" + \"-122.23\")\n",
    "\n",
    "#reverse Geocoding (Coordinates to Address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0aef79e-c901-45d5-8de7-4a2700065286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(coords, cache, geolocator_instance):\n",
    "    #Fetches location data (road, county) for coordinates, using a cache.\n",
    "    \n",
    "    if not geolocator_instance:\n",
    "        print(\"Geopy not available, cannot fetch location.\")\n",
    "        return {\"road\": None, \"county\": None}\n",
    "\n",
    "    lat, lon = map(str, coords)\n",
    "    coord_tuple = (lat, lon) # Use tuple as dict key\n",
    "\n",
    "    # Check cache first\n",
    "    if coord_tuple in cache:\n",
    "        # print(f\"Cache hit for {coord_tuple}\")\n",
    "        return cache[coord_tuple]\n",
    "\n",
    "     # Fetch from API if not in cache\n",
    "    print(f\"Cache miss for {coord_tuple}. Querying API...\")\n",
    "    try:\n",
    "        # Use addressdetails=True if available and needed for specific fields\n",
    "        location = geolocator_instance.reverse(coord_tuple, exactly_one=True, language='en')\n",
    "        address_data = location.raw.get('address', {}) if location else {}\n",
    "\n",
    "        road = address_data.get('road', None)\n",
    "        county = address_data.get('county', None)\n",
    "\n",
    "        loc_data = {\"road\": road, \"county\": county}\n",
    "        cache[coord_tuple] = loc_data # Update cache\n",
    "        # print(f\"Fetched & Cached: {coord_tuple} -> {loc_data}\")\n",
    "        time.sleep(1) \n",
    "\n",
    "    except GeocoderTimedOut:\n",
    "        print(f\"Warning: Geocoder timed out for {coord_tuple}. Returning None.\")\n",
    "        loc_data = {\"road\": None, \"county\": None}\n",
    "    except GeocoderServiceError as e:\n",
    "         print(f\"Warning: Geocoder service error for {coord_tuple}: {e}. Returning None.\")\n",
    "         loc_data = {\"road\": None, \"county\": None}\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: An unexpected error occurred during geocoding for {coord_tuple}: {e}. Returning None.\")\n",
    "        loc_data = {\"road\": None, \"county\": None}\n",
    "\n",
    "    return loc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fb68014-b3d6-4748-84ba-4488f3b41ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load/Save Cache \n",
    "def load_location_data(filepath):\n",
    "    \"\"\"Loads location cache from a pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            loc_update = pickle.load(f)\n",
    "            print(f\"Loaded location cache from {filepath}\")\n",
    "            return loc_update if isinstance(loc_update, dict) else {}\n",
    "    except (FileNotFoundError, EOFError):\n",
    "        print(f\"Cache file {filepath} not found or empty. Starting with an empty cache.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cache file {filepath}: {e}. Starting with an empty cache.\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdcd098e-e502-4489-8e0b-4a0bd4553ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_location_data(cache, filepath):\n",
    "    \"\"\"Saves location cache to a pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(cache, f)\n",
    "            print(f\"Saved location cache to {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cache file {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e377229e-1f2f-4f54-9dd9-3fd327f1ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempting to load location data from location_cache.pickle ---\n",
      "Cache file location_cache.pickle not found or empty. Starting with an empty cache.\n",
      "Could not load location data from cache. Location features ('road', 'county') will not be added.\n"
     ]
    }
   ],
   "source": [
    "if RUN_GEOCODING and GEOPY_LOADED and geolocator:\n",
    "    print(\"\\n--- Starting Geocoding Process (This will take a very long time!) ---\")\n",
    "    location_cache = load_location_data(LOCATION_CACHE_FILE) # Load existing data first\n",
    "\n",
    "    # Prepare coordinates, skipping those already in cache\n",
    "    coords_to_process = []\n",
    "    for lat, lon in df[['Latitude', 'Longitude']].values:\n",
    "         coord_tuple = (str(lat), str(lon))\n",
    "         if coord_tuple not in location_cache:\n",
    "             coords_to_process.append(coord_tuple)\n",
    "\n",
    "    print(f\"Total rows: {len(df)}. Cached entries: {len(location_cache)}. Need to fetch: {len(coords_to_process)}\")\n",
    "\n",
    "    location_results = []\n",
    "    start_time = time.time()\n",
    "    save_interval = 100 # Save progress every N iterations\n",
    "\n",
    "    for i, coord_tuple in enumerate(coords_to_process):\n",
    "        loc_data = get_location(coord_tuple, location_cache, geolocator)\n",
    "        location_results.append(loc_data) # Store result temporarily\n",
    "\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            # Update the main cache with newly fetched results before saving\n",
    "            newly_fetched_coords = coords_to_process[:i+1]\n",
    "            newly_fetched_results = location_results[:i+1]\n",
    "            for c, r in zip(newly_fetched_coords, newly_fetched_results):\n",
    "                 location_cache[c] = r # Ensure cache object is updated\n",
    "            save_location_data(location_cache, LOCATION_CACHE_FILE)\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Processed {i+1}/{len(coords_to_process)} new locations. Time elapsed: {elapsed:.2f}s. Cache saved.\")\n",
    "\n",
    "    # Final save after loop completes\n",
    "    newly_fetched_coords = coords_to_process\n",
    "    newly_fetched_results = location_results\n",
    "    for c, r in zip(newly_fetched_coords, newly_fetched_results):\n",
    "        location_cache[c] = r\n",
    "    save_location_data(location_cache, LOCATION_CACHE_FILE)\n",
    "    print(\"Geocoding process finished.\")\n",
    "\n",
    "    # Rebuild loc_df using the final cache in the correct order\n",
    "    all_location_data = [location_cache.get((str(lat), str(lon)), {\"road\": None, \"county\": None})\n",
    "                         for lat, lon in df[['Latitude', 'Longitude']].values]\n",
    "    loc_df = pd.DataFrame(all_location_data, index=df.index)\n",
    "\n",
    "\n",
    "elif not RUN_GEOCODING:\n",
    "    print(f\"\\n--- Attempting to load location data from {LOCATION_CACHE_FILE} ---\")\n",
    "    location_cache = load_location_data(LOCATION_CACHE_FILE)\n",
    "    if location_cache:\n",
    "         # Reconstruct DataFrame from cache, ensuring order matches df\n",
    "         ordered_location_data = [location_cache.get((str(lat), str(lon)), {\"road\": None, \"county\": None})\n",
    "                                  for lat, lon in df[['Latitude', 'Longitude']].values]\n",
    "         loc_df = pd.DataFrame(ordered_location_data, index=df.index)\n",
    "         print(f\"Successfully created DataFrame from cached location data. Found {len(loc_df)} entries.\")\n",
    "         # Check if the reconstructed df has expected columns and length\n",
    "         if not all(col in loc_df.columns for col in ['road', 'county']):\n",
    "              print(\"Warning: Loaded cache seems incomplete (missing 'road' or 'county'). Location features might be unusable.\")\n",
    "              loc_df = pd.DataFrame() # Reset if invalid\n",
    "         elif len(loc_df) != len(df):\n",
    "              print(f\"Warning: Cache length ({len(loc_df)}) does not match main DataFrame length ({len(df)}). Skipping merge.\")\n",
    "              loc_df = pd.DataFrame() # Reset if invalid\n",
    "    else:\n",
    "        print(\"Could not load location data from cache. Location features ('road', 'county') will not be added.\")\n",
    "        loc_df = pd.DataFrame() # Ensure it's empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30f0b683-084f-4b5d-a2ee-164b948081c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Skipping location data merge ---\n"
     ]
    }
   ],
   "source": [
    "# --- Merge Location Data (if available) ---\n",
    "if not loc_df.empty:\n",
    "    print(\"\\n--- Merging location data into main DataFrame ---\")\n",
    "    # Ensure indices align before concatenation\n",
    "    df = pd.concat([df, loc_df[['road', 'county']]], axis=1)\n",
    "    print(\"Location data merged.\")\n",
    "    # print(\"\\nDataFrame head after merging location:\")\n",
    "    # print(df.head())\n",
    "    # print(\"\\nDataFrame info after merging location:\")\n",
    "    # df.info()\n",
    "else:\n",
    "    print(\"\\n--- Skipping location data merge ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e1cbe-0a93-411a-bdf8-220e0452f53f",
   "metadata": {},
   "source": [
    "##  Drop Original Latitude/Longitude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eee9ee93-0003-4bf4-9843-d3184ad284c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dropping Latitude and Longitude columns ---\n",
      "Dropped columns: ['Latitude', 'Longitude']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Dropping Latitude and Longitude columns ---\")\n",
    "cols_to_drop = ['Latitude', 'Longitude']\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "if existing_cols_to_drop:\n",
    "    df = df.drop(labels=existing_cols_to_drop, axis=1)\n",
    "    print(f\"Dropped columns: {existing_cols_to_drop}\")\n",
    "else:\n",
    "    print(\"Latitude/Longitude columns not found or already dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e14e182-1ffe-4f11-8e7f-e4edff318b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'road' column not found, skipping imputation and encoding.\n"
     ]
    }
   ],
   "source": [
    "#  Handle Missing Values in 'road' and 'county' (using Mode Imputation) \n",
    "# This step is crucial if geocoding failed for some entries or cache was incomplete\n",
    "\n",
    "\n",
    "if 'road' in df.columns:\n",
    "    if df['road'].isnull().any():\n",
    "        print(\"\\n--- Imputing missing values in 'road' column using mode ---\")\n",
    "        road_mode = df['road'].mode()[0]\n",
    "        df['road'].fillna(road_mode, inplace=True)\n",
    "        print(f\"Missing 'road' values filled with: {road_mode}\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values found in 'road' column.\")\n",
    "else:\n",
    "     print(\"\\n'road' column not found, skipping imputation and encoding.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7274366-623d-4b49-b92f-3c7e9726b813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'county' column not found, skipping imputation and encoding.\n"
     ]
    }
   ],
   "source": [
    "if 'county' in df.columns:\n",
    "    if df['county'].isnull().any():\n",
    "        print(\"\\n--- Imputing missing values in 'county' column using mode ---\")\n",
    "        county_mode = df['county'].mode()[0]\n",
    "        df['county'].fillna(county_mode, inplace=True)\n",
    "        print(f\"Missing 'county' values filled with: {county_mode}\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values found in 'county' column.\")\n",
    "else:\n",
    "     print(\"\\n'county' column not found, skipping imputation and encoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f8d272-1974-4ea0-bad0-d7d5f0340053",
   "metadata": {},
   "source": [
    "##  Label Encode Categorical Features ('road', 'county') \n",
    "Label encoding is a technique used in machine learning and data analysis to convert categorical variables into numerical format.\n",
    "\n",
    "### Initialize encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cfbea99-d4ca-4aa1-8836-35bdf8a48011",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_road = LabelEncoder()\n",
    "le_county = LabelEncoder()\n",
    "ROAD_ENCODER_FILE = 'le_road_encoder.pkl'\n",
    "COUNTY_ENCODER_FILE = 'le_county_encoder.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6aa2382-1f67-445f-8bdd-dbe775fec756",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'road' in df.columns:\n",
    "    print(\"\\n--- Label Encoding 'road' column ---\")\n",
    "    df['road'] = le_road.fit_transform(df['road'])\n",
    "    # Save the encoder\n",
    "    with open(ROAD_ENCODER_FILE, 'wb') as f:\n",
    "        pickle.dump(le_road, f)\n",
    "    print(f\"'road' column encoded. Encoder saved to {ROAD_ENCODER_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "483451cb-085e-462f-ab66-d7b1d5115c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'county' in df.columns:\n",
    "    print(\"\\n--- Label Encoding 'county' column ---\")\n",
    "    df['county'] = le_county.fit_transform(df['county'])\n",
    "    # Save the encoder\n",
    "    with open(COUNTY_ENCODER_FILE, 'wb') as f:\n",
    "        pickle.dump(le_county, f)\n",
    "    print(f\"'county' column encoded. Encoder saved to {COUNTY_ENCODER_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a5f4872-e2ce-4918-af08-b1d80d0c5bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final DataFrame before splitting ---\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Target\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556   4.526\n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842   3.585\n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260   3.521\n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945   3.413\n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467   3.422\n",
      "\n",
      "Final DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   MedInc      20640 non-null  float64\n",
      " 1   HouseAge    20640 non-null  float64\n",
      " 2   AveRooms    20640 non-null  float64\n",
      " 3   AveBedrms   20640 non-null  float64\n",
      " 4   Population  20640 non-null  float64\n",
      " 5   AveOccup    20640 non-null  float64\n",
      " 6   Target      20640 non-null  float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final DataFrame before splitting ---\")\n",
    "print(df.head())\n",
    "print(\"\\nFinal DataFrame info:\")\n",
    "df.info() # Check for NaNs and data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a25d6f-a43a-4eb7-bc3e-ba22f7151f17",
   "metadata": {},
   "source": [
    "##   Prepare Data for Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7782c5d-64b7-4394-af39-841c3a865716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing data for modeling ---\n",
      "Features (X shape): (20640, 6)\n",
      "Target (y shape): (20640,)\n",
      "Feature columns used for training: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preparing data for modeling ---\")\n",
    "if 'Target' not in df.columns:\n",
    "     raise ValueError(\"Target column is missing from the DataFrame before splitting.\")\n",
    "\n",
    "y = df['Target'].values\n",
    "X = df.drop('Target', axis=1)\n",
    "X_cols = X.columns.tolist() # Store column names/order for prediction consistency\n",
    "X = X.values # Convert to numpy array\n",
    "\n",
    "print(f\"Features (X shape): {X.shape}\")\n",
    "print(f\"Target (y shape): {y.shape}\")\n",
    "print(f\"Feature columns used for training: {X_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00b0b7-c220-43f2-8cd4-eca1198e0521",
   "metadata": {},
   "source": [
    "##  Train-Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4a5930e-0530-405f-af65-54d7f3381c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting data into Training and Test sets ---\n",
      "X_train shape: (16512, 6)\n",
      "X_test shape: (4128, 6)\n",
      "y_train shape: (16512,)\n",
      "y_test shape: (4128,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Splitting data into Training and Test sets ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce460b9-c868-49f0-b264-c190527b7046",
   "metadata": {},
   "source": [
    "##  Model Training (Random Forest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53d7dcbc-7e8c-4dbc-b00c-de4f0abb489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training RandomForestRegressor model ---\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Training RandomForestRegressor model ---\")\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1) # Example parameters\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ef1c8-b55a-418c-a2a4-53bdb585fa56",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "547caeec-552d-49fa-92a5-884ba952b912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Making predictions on the test set ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Making predictions on the test set ---\")\n",
    "y_pred = model.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1be7dd-ac1f-4221-baf2-e709676a2fd5",
   "metadata": {},
   "source": [
    "## Check Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6613d09e-c122-401e-8e6d-705efc00acd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating model performance ---\n",
      "Model R2 Score on Test Set: 0.6772 (67.72%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluating model performance ---\")\n",
    "accuracy = r2_score(y_test, y_pred)\n",
    "print(f\"Model R2 Score on Test Set: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca28de1-74d8-40d8-a51f-f35550b4f7c7",
   "metadata": {},
   "source": [
    "## Predict on Custom/New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be6e62d6-06d4-40bd-ac33-d5d54098cefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example: Predicting on new data ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Example: Predicting on new data ---\")\n",
    "\n",
    "# 1. Define new data with raw categorical values\n",
    "# Use column names matching the original df before Target was dropped\n",
    "raw_input_data = {\n",
    "    'MedInc': 8.3252,       # Example value from the dataset\n",
    "    'HouseAge': 41.0,\n",
    "    'AveRooms': 6.984127,\n",
    "    'AveBedrms': 1.023810,\n",
    "    'Population': 322.0,\n",
    "    'AveOccup': 2.555556,\n",
    "    # --- Categorical features need to exist if they were used in training ---\n",
    "    'county': 'Alameda County',  # Example: Must be a county seen during training\n",
    "    'road': 'Tunnel Road'      # Example: Must be a road seen during training\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "239b1ea3-1f90-4dca-b5d4-cd6df78d800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add placeholders for columns if they weren't created (e.g., if geocoding failed/skipped)\n",
    "if 'county' not in X_cols:\n",
    "     del raw_input_data['county']\n",
    "if 'road' not in X_cols:\n",
    "     del raw_input_data['road']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f742249-70f1-4324-8af8-18c10f119d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the saved Label Encoders\n",
    "\n",
    "encoders_loaded = True\n",
    "try:\n",
    "    if 'county' in raw_input_data:\n",
    "        with open(COUNTY_ENCODER_FILE, 'rb') as f:\n",
    "            le_county_loaded = pickle.load(f)\n",
    "        print(f\"Loaded County encoder from {COUNTY_ENCODER_FILE}\")\n",
    "    if 'road' in raw_input_data:\n",
    "         with open(ROAD_ENCODER_FILE, 'rb') as f:\n",
    "            le_road_loaded = pickle.load(f)\n",
    "         print(f\"Loaded Road encoder from {ROAD_ENCODER_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not load saved encoders. Cannot transform new categorical data.\")\n",
    "    encoders_loaded = False\n",
    "except Exception as e:\n",
    "    print(f\"Error loading encoders: {e}\")\n",
    "    encoders_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2cddd0a-2971-4140-abc6-aefeb003d899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Data (Processed):\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup\n",
      "0  8.3252      41.0  6.984127    1.02381       322.0  2.555556\n",
      "\n",
      "Predicted Median House Value: $473,342.77\n"
     ]
    }
   ],
   "source": [
    "if encoders_loaded or ('county' not in raw_input_data and 'road' not in raw_input_data):\n",
    "    # 3. Create a DataFrame and Transform Categorical Features\n",
    "    inp_df = pd.DataFrame([raw_input_data])\n",
    "    prediction_possible = True\n",
    "\n",
    "    try:\n",
    "        if 'county' in inp_df.columns:\n",
    "             # Check if label is known before transforming\n",
    "             county_label = inp_df['county'].iloc[0]\n",
    "             if county_label not in le_county_loaded.classes_:\n",
    "                  print(f\"Warning: County '{county_label}' was not seen during training. Prediction might be inaccurate.\")\n",
    "                  \n",
    "             inp_df['county'] = le_county_loaded.transform(inp_df['county'])\n",
    "\n",
    "        if 'road' in inp_df.columns:\n",
    "             road_label = inp_df['road'].iloc[0]\n",
    "             if road_label not in le_road_loaded.classes_:\n",
    "                   print(f\"Warning: Road '{road_label}' was not seen during training. Prediction might be inaccurate.\")\n",
    "             inp_df['road'] = le_road_loaded.transform(inp_df['road'])\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error transforming new data: {e}. This usually means an unknown category was provided.\")\n",
    "        prediction_possible = False\n",
    "    except NameError as e:\n",
    "         # This happens if an encoder wasn't loaded but the column exists\n",
    "         print(f\"Error: Encoder not loaded for a required column. {e}\")\n",
    "         prediction_possible = False\n",
    "\n",
    "\n",
    "    if prediction_possible:\n",
    "        # 4. Ensure Feature Order Matches Training Data (X_cols)\n",
    "        try:\n",
    "            inp_final = inp_df[X_cols].values # Select columns in the correct order\n",
    "        except KeyError as e:\n",
    "             print(f\"Error: Missing expected column in input data: {e}\")\n",
    "             inp_final = None\n",
    "             prediction_possible = False\n",
    "\n",
    "    if prediction_possible and inp_final is not None:\n",
    "        # 5. Make Prediction\n",
    "        new_prediction_scaled = model.predict(inp_final)\n",
    "        new_prediction_dollars = new_prediction_scaled[0] * 100000 # Scale back to dollars\n",
    "\n",
    "        print(f\"\\nInput Data (Processed):\")\n",
    "        print(inp_df[X_cols]) # Show the numeric data fed to the model\n",
    "        print(f\"\\nPredicted Median House Value: ${new_prediction_dollars:,.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping prediction on new data due to issues with loading encoders or transforming input.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac11d3e-35eb-4261-b351-9ea370706fa5",
   "metadata": {},
   "source": [
    "### Manual Input Prediction Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4f455599-43ca-4370-89f2-71f337bfde2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Enter values below to predict house price ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter value for 'MedInc':  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Invalid input. 'MedInc' requires a numeric value.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter value for 'MedInc':  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Invalid input. 'MedInc' requires a numeric value.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# Get input for the specific feature\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m         val_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter value for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# Convert to float - handle potential errors\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(val_str)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1267\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# First, check if the model and necessary variables exist \n",
    "\n",
    "\n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"Error: Model has not been trained yet. Please run the training cells.\")\n",
    "elif 'X_cols' not in locals() or not X_cols:\n",
    "    print(\"Error: Feature list 'X_cols' used for training is not defined.\")\n",
    "else:\n",
    "    print(\"\\n--- Enter values below to predict house price ---\")\n",
    "    # # Inform the user which features are needed\n",
    "    # print(f\"(Model expects the following {len(X_cols)} features: {X_cols})\")\n",
    "\n",
    "    input_features = {}\n",
    "    prediction_possible = True\n",
    "\n",
    "    # Iterate over the columns the model was ACTUALLY trained on\n",
    "    for feature in X_cols:\n",
    "        while True:\n",
    "            try:\n",
    "                # Get input for the specific feature\n",
    "                val_str = input(f\"Enter value for '{feature}': \")\n",
    "                # Convert to float - handle potential errors\n",
    "                val = float(val_str)\n",
    "                input_features[feature] = val\n",
    "                break # Exit inner loop (while True) on successful input\n",
    "            except ValueError:\n",
    "                print(f\"  Invalid input. '{feature}' requires a numeric value.\")\n",
    "            except EOFError:\n",
    "                 print(\"\\nInput interrupted. Cannot predict.\")\n",
    "                 prediction_possible = False\n",
    "                 break # Exit inner loop (while True)\n",
    "            except Exception as e:\n",
    "                print(f\"  An unexpected error occurred during input: {e}\")\n",
    "                # Decide if you want to retry or abort\n",
    "                retry = input(\"  Retry input for this feature? (y/n): \").lower()\n",
    "                if retry != 'y':\n",
    "                    prediction_possible = False\n",
    "                    break # Exit inner loop (while True)\n",
    "        if not prediction_possible:\n",
    "             break # Exit outer loop (for feature in X_cols)\n",
    "\n",
    "    # --- Proceed only if all inputs were collected successfully ---\n",
    "    if prediction_possible and len(input_features) == len(X_cols):\n",
    "        # Convert collected features to a DataFrame\n",
    "        # The keys in input_features automatically become column names\n",
    "        user_input_df = pd.DataFrame([input_features])\n",
    "\n",
    "        # --- Reorder columns to match the exact training order (CRITICAL) ---\n",
    "        # Even if the dictionary likely preserves order, explicit reordering is safer.\n",
    "        try:\n",
    "            user_input_df = user_input_df[X_cols]\n",
    "        except KeyError as e:\n",
    "             print(f\"\\nError: Mismatch between input features and expected columns: {e}\")\n",
    "             print(\"Cannot proceed with prediction.\")\n",
    "             prediction_possible = False # Redundant but clear\n",
    "\n",
    "        if prediction_possible:\n",
    "            try:\n",
    "                # Predict using the model\n",
    "                # The input DataFrame now has the correct 6 features in the correct order\n",
    "                predicted_price_scaled = model.predict(user_input_df)[0]\n",
    "\n",
    "                # Scale the prediction back to dollars (assuming target was in $100,000s)\n",
    "                predicted_price_dollars = predicted_price_scaled * 100000\n",
    "\n",
    "                print(\"-\" * 30) # Separator\n",
    "                print(f\"\\n Predicted Median House Value: ${predicted_price_dollars:,.2f} \")\n",
    "                print(\"-\" * 30) # Separator\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during model prediction: {e}\")\n",
    "\n",
    "    elif prediction_possible:\n",
    "        # This case happens if the outer loop finished but not all features were collected (shouldn't happen with current logic, but good check)\n",
    "        print(\"\\nError: Did not collect the correct number of features. Cannot predict.\")\n",
    "    # else: (prediction_possible is False - error message already printed during input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819afb16-3f50-44b5-92bb-c35a18aba132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
